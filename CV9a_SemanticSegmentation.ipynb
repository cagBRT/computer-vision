{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV9a SemanticSegmentation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOZAzIIx1oF1osn2N/0RID+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV9a_SemanticSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2q9RoqYbAY3"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGBpNHQhabAF"
      },
      "source": [
        "# **Semantic Segmentation**\n",
        "This notebook uses PyTorch to perform semantic segmentation on images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORMgB2gtapXF"
      },
      "source": [
        "The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because we're predicting for every pixel in the image, this task is commonly referred to as dense prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGp3_76nbnKi"
      },
      "source": [
        "One important thing to note is that we're not separating instances of the same class; we only care about the category of each pixel. In other words, if you have two objects of the same category in your input image, the segmentation map does not inherently distinguish these as separate objects. There exists a different class of models, known as instance segmentation models, which do distinguish between separate objects of the same class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRP-Ayca0pv"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread(\"/content/cloned-repo/images/bikeRiders.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWSZfY_LcR1F"
      },
      "source": [
        "image = cv2.imread(\"/content/semanticSegmentationArch2.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4e-c49MLXDa"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpLcRcuWLg8_"
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(\"/content/Navajo_people_and_sheep.jpg\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB3hME6kLn5z"
      },
      "source": [
        "# create a color pallette, selecting a color for each class\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# plot the semantic segmentation predictions of 21 classes in each color\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(r)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z96pNTkVYuSW"
      },
      "source": [
        "plt.imshow(input_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}