{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV9a SemanticSegmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPel/cNFHpyc1XTT1/2QUxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV9a_SemanticSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2q9RoqYbAY3"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubhXcI1SinS2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGBpNHQhabAF"
      },
      "source": [
        "# **Semantic Segmentation**\n",
        "This notebook uses PyTorch to perform semantic segmentation on images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORMgB2gtapXF"
      },
      "source": [
        "The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because we're predicting for every pixel in the image, this task is commonly referred to as dense prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGp3_76nbnKi"
      },
      "source": [
        "One important thing to note is that *we're not separating instances of the same class; we only care about the category of each pixel*. In other words, if you have two objects of the same category in your input image, the segmentation map does not inherently distinguish these as separate objects. There exists a different class of models, known as *instance segmentation models, which do distinguish between separate objects of the same class*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeRP-Ayca0pv"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread(\"/content/cloned-repo/images/bikeRiders.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWSZfY_LcR1F"
      },
      "source": [
        "image = cv2.imread(\"/content/cloned-repo/images/semanticSegmentationArch2.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TowYjsw_18Xu"
      },
      "source": [
        "**Pytorch Hub**<br>\n",
        "Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIpWLcFC2lIX"
      },
      "source": [
        "All of the pre-trained models expect input images normalized in the same way,<br> i.e. mini-batches of 3-channel RGB images of shape (N, 3, H, W), where N is the number of images, H and W are expected to be at least 224 pixels. <br>\n",
        "The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
        "\n",
        "The model returns an OrderedDict with two Tensors that are of the same height and width as the input Tensor, but with 21 classes. output['out'] contains the semantic masks, and output['aux'] contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful. So, output['out'] is of shape (N, 21, H, W).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoyNOJR_DKrD"
      },
      "source": [
        "**The model**<br>\n",
        "DeepLabV3 model with a ResNet-101 backbone.<br>\n",
        "Deeplabv3-ResNet101 is constructed by a Deeplabv3 model with a ResNet-101 backbone. <br>\n",
        "\n",
        "The pre-trained model has been trained on a subset of [COCO train2017](https://cocodataset.org/#explore), on the 20 categories that are present in the Pascal VOC dataset<br>\n",
        "\n",
        "This model has a Global Pixelwise Accuracy of 92.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClSjs5FkEWIY"
      },
      "source": [
        "**Backbone**<br>\n",
        "Backbone is a term used in DeepLab models/papers to refer to the feature extractor network. These feature extractor networks compute features from the input image and then these features are upsampled by a simple decoder module of DeepLab models to generate segmented masks. The authors of DeepLab models have shown performance with different feature extractors (backbones) like MobileNet, ResNet, and Xception network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4e-c49MLXDa"
      },
      "source": [
        "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq2SOg9327W1"
      },
      "source": [
        "**Select an image and do the preprocessing**<br>\n",
        "1. Normalize the pixel values in the image\n",
        "2. Modify the format of the image to one expected by the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaFizKaz24dN"
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "\n",
        "input_image = Image.open(\"images/Navajo_people_and_sheep.jpg\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdHqMi5W31pz"
      },
      "source": [
        "**Prepare for GPU usage**<br>\n",
        "Computer vision models are compuationally intense, using GPUs reduces processing time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpLcRcuWLg8_"
      },
      "source": [
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8au2as74ZH6"
      },
      "source": [
        "**No Grad**<br>\n",
        "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYih0buO4ZRY"
      },
      "source": [
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePufTmln4kwq"
      },
      "source": [
        "**The Color Palette**<br>\n",
        "Select the color palette and colors for each class.<br>\n",
        "Could also use PaletteNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngpB1kGU40m3"
      },
      "source": [
        "# create a color pallette, selecting a color for each class\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbGEkRLk5Rkb"
      },
      "source": [
        "**Plot the semantic segmentation of the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB3hME6kLn5z"
      },
      "source": [
        "# plot the semantic segmentation predictions of 21 classes in each color\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "plt.imshow(r)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC0hvq2K5ekA"
      },
      "source": [
        "**Compare the segmented image to the original image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z96pNTkVYuSW"
      },
      "source": [
        "plt.imshow(input_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuFHkMtD1hWX"
      },
      "source": [
        "# **Assignment**\n",
        "Pick an image in the /content/cloned-repo/images directory and apply the semantic segmentation algorithm on it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRCTc4O614VY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}