{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV9b SemanticSegmentationDeepLab.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMU9unids+WyxEesTtfpf4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV9b_SemanticSegmentationDeepLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lKFzme97GxE"
      },
      "source": [
        "This notebook uses Google's TPUs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZlKHQLa_riI"
      },
      "source": [
        "# **DeepLab**<br>\n",
        "DeepLab is a state-of-art deep learning model for semantic image segmentation, where the goal is to assign semantic labels (e.g., person, dog, cat and so on) to every pixel in the input image. <br>\n",
        "https://github.com/tensorflow/models/blob/master/research/deeplab/README.md\n",
        "\n",
        "This notebook demonstrates the steps to use the DeepLab model to perform semantic segmentation on a sample input image. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHpItsKQ7k6Y"
      },
      "source": [
        "**Import Libraries**<br>\n",
        "Note- Tensorflow version 1.x is used in this notebook. <br>\n",
        "It is not desirable to use Tensorflow 1.x as opposed to Tensorflow 2.x. \n",
        "The reason this notebook uses 1.x is there are issues with Tensorflow 2.x and a number of computer vision libraries and models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOgMHolT7Q0O"
      },
      "source": [
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6J51QRw8E-7"
      },
      "source": [
        "## Import helper methods\n",
        "These methods help us perform the following tasks:\n",
        "* Load the latest version of the pretrained DeepLab model\n",
        "* Load the colormap from the PASCAL VOC dataset\n",
        "* Adds colors to various labels, such as \"pink\" for people, \"green\" for bicycle and more\n",
        "* Visualize an image, and add an overlay of colors on various regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zo2g1Dz8FJM",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Functions\n",
        "class DeepLabModel(object):\n",
        "  \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "  INPUT_SIZE = 513\n",
        "  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "  def __init__(self, tarball_path):\n",
        "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "    self.graph = tf.Graph()\n",
        "\n",
        "    graph_def = None\n",
        "    # Extract frozen graph from tar archive.\n",
        "    tar_file = tarfile.open(tarball_path)\n",
        "    for tar_info in tar_file.getmembers():\n",
        "      if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "        file_handle = tar_file.extractfile(tar_info)\n",
        "        graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "        break\n",
        "\n",
        "    tar_file.close()\n",
        "\n",
        "    if graph_def is None:\n",
        "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "    with self.graph.as_default():\n",
        "      tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "  def run(self, image):\n",
        "    \"\"\"Runs inference on a single image.\n",
        "\n",
        "    Args:\n",
        "      image: A PIL.Image object, raw input image.\n",
        "\n",
        "    Returns:\n",
        "      resized_image: RGB image resized from original input image.\n",
        "      seg_map: Segmentation map of `resized_image`.\n",
        "    \"\"\"\n",
        "    width, height = image.size\n",
        "    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
        "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "    batch_seg_map = self.sess.run(\n",
        "        self.OUTPUT_TENSOR_NAME,\n",
        "        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "    seg_map = batch_seg_map[0]\n",
        "    return resized_image, seg_map\n",
        "\n",
        "\n",
        "def create_pascal_label_colormap():\n",
        "  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
        "\n",
        "  Returns:\n",
        "    A Colormap for visualizing segmentation results.\n",
        "  \"\"\"\n",
        "  colormap = np.zeros((256, 3), dtype=int)\n",
        "  ind = np.arange(256, dtype=int)\n",
        "\n",
        "  for shift in reversed(range(8)):\n",
        "    for channel in range(3):\n",
        "      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
        "    ind >>= 3\n",
        "\n",
        "  return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "  \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "  Args:\n",
        "    label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "  Returns:\n",
        "    result: A 2D array with floating type. The element of the array\n",
        "      is the color indexed by the corresponding element in the input label\n",
        "      to the PASCAL color map.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If label is not of rank 2 or its value is larger than color\n",
        "      map maximum entry.\n",
        "  \"\"\"\n",
        "  if label.ndim != 2:\n",
        "    raise ValueError('Expect 2-D input label')\n",
        "\n",
        "  colormap = create_pascal_label_colormap()\n",
        "\n",
        "  if np.max(label) >= len(colormap):\n",
        "    raise ValueError('label value too large.')\n",
        "\n",
        "  return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "  plt.subplot(grid_spec[0])\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.title('input image')\n",
        "\n",
        "  plt.subplot(grid_spec[1])\n",
        "  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "  plt.imshow(seg_image)\n",
        "  plt.axis('off')\n",
        "  plt.title('segmentation map')\n",
        "\n",
        "  plt.subplot(grid_spec[2])\n",
        "  plt.imshow(image)\n",
        "  plt.imshow(seg_image, alpha=0.7)\n",
        "  plt.axis('off')\n",
        "  plt.title('segmentation overlay')\n",
        "\n",
        "  unique_labels = np.unique(seg_map)\n",
        "  ax = plt.subplot(grid_spec[3])\n",
        "  plt.imshow(\n",
        "      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "  ax.yaxis.tick_right()\n",
        "  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "  plt.xticks([], [])\n",
        "  ax.tick_params(width=0.0)\n",
        "  plt.grid('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "])\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
        "\n",
        "def run_visualization(url):\n",
        "  \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n",
        "  try:\n",
        "    f = urllib.request.urlopen(url)\n",
        "    jpeg_str = f.read()\n",
        "    original_im = Image.open(BytesIO(jpeg_str))\n",
        "  except IOError:\n",
        "    print('Cannot retrieve image. Please check url: ' + url)\n",
        "    return\n",
        "\n",
        "  print('running deeplab on image %s...' % url)\n",
        "  resized_im, seg_map = MODEL.run(original_im)\n",
        "\n",
        "  vis_segmentation(resized_im, seg_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWmRO3pP8bS1"
      },
      "source": [
        "## Select pretrained models\n",
        "The DeepLab model has been trained using four different backbone networks. <br>\n",
        "Each model will be used and the results compared.<br>\n",
        "- **mobilenetv2**: Lightweight deep learning model for semantic image segmentation. <br>\n",
        "It uses the mobilenetv2_coco_voc_train architecture<br>\n",
        "It is trained on the Pascal VOC 2012 dataset<br>\n",
        "\n",
        "- **mobilenetv2_coco_voctrainaug**: Lightweight deep learning model for semantic image segmentation. <br>\n",
        "It uses the mobilenetv2_coco_voc_train architecture<br>\n",
        "It is trained on the augmented Pascal VOC 2012 dataset<br>\n",
        "\n",
        "- **xception_coco_voctrainaug**: a model based on the network backbone Xception using the pretrained COCO arichtecture and the Pascal VOC 2012 dataset<br>\n",
        "\n",
        "- **xception_coco_voctrainaug**: a model based on the network backbone Xception using the pretrained COCO arichtecture and the augmented Pascal VOC 2012 dataset<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "excfVsZpA2ft"
      },
      "source": [
        "**Download the selected model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYU4IxsF8Yye",
        "cellView": "code"
      },
      "source": [
        "MODEL_NAME  = 'mobilenetv2_coco_voctrainval' \n",
        "MODEL_NAME2 = 'mobilenetv2_coco_voctrainaug'\n",
        "MODEL_NAME3 = 'xception_coco_voctrainaug'\n",
        "MODEL_NAME4 = 'xception_coco_voctrainval'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_voctrainaug':\n",
        "        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
        "    'mobilenetv2_coco_voctrainval':\n",
        "        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
        "    'xception_coco_voctrainaug':\n",
        "        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
        "    'xception_coco_voctrainval':\n",
        "        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzz5lxZO8tuT"
      },
      "source": [
        "## Run on a sample image\n",
        "\n",
        "Run all four models on the same image and compare the output for each model. \n",
        "\n",
        "(https://github.com/tensorflow/models/blob/master/research/deeplab/README.md) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGHRLKXmDLmo"
      },
      "source": [
        "**Using the mobilenetv2_coco_voctrainval model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kltlPEHKCTNu"
      },
      "source": [
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar8d2fCG8uo4",
        "cellView": "code"
      },
      "source": [
        "run_visualization(\"https://github.com/cagBRT/computer-vision/blob/master/images/Navajo_people_and_sheep.jpg?raw=true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbxI0t3EUr8"
      },
      "source": [
        "**Using the mobilenetv2_coco_voctrainaug model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYvFHEt-9y0N"
      },
      "source": [
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME2],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXLNESem-SzS"
      },
      "source": [
        "run_visualization(\"https://github.com/cagBRT/computer-vision/blob/master/images/Navajo_people_and_sheep.jpg?raw=true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEpIccI0ElU0"
      },
      "source": [
        "**Using the xception_coco_voctrainval model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuVTrvzg-kPY"
      },
      "source": [
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME4],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-snaa0w-owK"
      },
      "source": [
        "run_visualization(\"https://github.com/cagBRT/computer-vision/blob/master/images/Navajo_people_and_sheep.jpg?raw=true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNfEgH4lEd4O"
      },
      "source": [
        "**Using the xception_coco_voctrainaug model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbZPopKx-Xsy"
      },
      "source": [
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME3],\n",
        "                   download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFBXpHTF-cXH"
      },
      "source": [
        "run_visualization(\"https://github.com/cagBRT/computer-vision/blob/master/images/Navajo_people_and_sheep.jpg?raw=true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmeiUt_DFKTN"
      },
      "source": [
        "# **Assignment** \n",
        "1.Select an image from https://github.com/cagBRT/computer-vision/tree/master/images\n",
        "2. Select \"Download\"\n",
        "3. Copy the url \n",
        "4. Paste the url into the code and add ?raw=true to the end of the url\n",
        "5. Run the four different segmentation models on your image\n"
      ]
    }
  ]
}