{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ContentBasedImageRetrieval.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNeHlDd+erlQCIUF3ZH5O/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/ContentBasedImageRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIspgZevcMUE"
      },
      "source": [
        "CBIR:<br>\n",
        "Phase #1: Train the autoencoder<br>\n",
        "Phase #2: Extract features from all images in our dataset by computing their latent-space representations using the autoencoder<br>\n",
        "Phase #3: Compare latent-space vectors to find all relevant images in the dataset<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2SG7SZicEva"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Vhbe1tcyVE"
      },
      "source": [
        "class ConvAutoencoder:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\t\t# initialize the input shape to be \"channels last\" along with\n",
        "\t\t# the channels dimension itself\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\t\t# define the input to the encoder\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = inputs\n",
        "\t\t# loop over the number of filters\n",
        "\t\tfor f in filters:\n",
        "\t\t\t# apply a CONV => RELU => BN operation\n",
        "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\t# flatten the network and then construct our latent vector\n",
        "\t\tvolumeSize = K.int_shape(x)\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tlatent = Dense(latentDim, name=\"encoded\")(x)\n",
        "# start building the decoder model which will accept the\n",
        "\t\t# output of the encoder as its inputs\n",
        "\t\tx = Dense(np.prod(volumeSize[1:]))(latent)\n",
        "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\t\t# loop over our number of filters again, but this time in\n",
        "\t\t# reverse order\n",
        "\t\tfor f in filters[::-1]:\n",
        "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
        "\t\t\t\tpadding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
        "\t\t# original depth of the image\n",
        "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "\t\toutputs = Activation(\"sigmoid\", name=\"decoded\")(x)\n",
        "\t\t# construct our autoencoder model\n",
        "\t\tautoencoder = Model(inputs, outputs, name=\"autoencoder\")\n",
        "\t\t# return the autoencoder model\n",
        "\t\treturn autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoNGguyec7Cr"
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGVSIIhLc7FQ"
      },
      "source": [
        "def visualize_predictions(decoded, gt, samples=10):\n",
        "\t# initialize our list of output images\n",
        "\toutputs = None\n",
        "\t# loop over our number of output samples\n",
        "\tfor i in range(0, samples):\n",
        "\t\t# grab the original image and reconstructed image\n",
        "\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n",
        "\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\t\t# stack the original and reconstructed image side-by-side\n",
        "\t\toutput = np.hstack([original, recon])\n",
        "\t\t# if the outputs array is empty, initialize it as the current\n",
        "\t\t# side-by-side image display\n",
        "\t\tif outputs is None:\n",
        "\t\t\toutputs = output\n",
        "\t\t# otherwise, vertically stack the outputs\n",
        "\t\telse:\n",
        "\t\t\toutputs = np.vstack([outputs, output])\n",
        "\t# return the output images\n",
        "\treturn outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDPLxEhG31CW"
      },
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 20\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "# load the MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "autoencoder = ConvAutoencoder.build(28, 28, 1)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit(\n",
        "\ttrainX, trainX,\n",
        "\tvalidation_data=(testX, testX),\n",
        "\tepochs=EPOCHS,\n",
        "\tbatch_size=BS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvxXMULQ4Igv"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "# use the convolutional autoencoder to make predictions on the\n",
        "# testing images, construct the visualization, and then save it\n",
        "# to disk\n",
        "print(\"[INFO] making predictions...\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "vis = visualize_predictions(decoded, testX)\n",
        "#cv2.imwrite('imageTest.jpg', vis)\n",
        "cv2_imshow(vis)\n",
        "\n",
        "# serialize the autoencoder model to disk\n",
        "#print(\"[INFO] saving autoencoder...\")\n",
        "#autoencoder.save(args[\"model\"], save_format=\"h5\")\n",
        "#autoencoder.save(\"modelOut\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fbLnqMPIL9-"
      },
      "source": [
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, EPOCHS)\n",
        "#plt.style.use(\"ggplot\")\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "#plt.savefig(\"plot.jpg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}