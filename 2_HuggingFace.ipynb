{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPwZIRmXPM/4xksqWJXT5Lw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/2_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl1Va9N0Akml"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data\n",
        "Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:\n",
        "\n",
        ">Preprocess textual data with a tokenizer.<br>\n",
        "Preprocess image or audio data with a feature extractor.<br>\n",
        "Preprocess data for a multimodal task with a processor.<br>"
      ],
      "metadata": {
        "id": "eSahQsoNAueb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP"
      ],
      "metadata": {
        "id": "1dXn2lkpA4ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main tool for processing textual data is a tokenizer. A tokenizer starts by splitting text into tokens according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.\n",
        "\n",
        "**If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer.** This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the vocab) during pretraining.\n",
        "\n",
        "Get started quickly by loading a pretrained tokenizer with the AutoTokenizer class. This downloads the vocab used when a model is pretrained.\n",
        "\n"
      ],
      "metadata": {
        "id": "L86Hez5rA-kP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize\n",
        "Load a pretrained tokenizer with AutoTokenizer.from_pretrained():"
      ],
      "metadata": {
        "id": "C-rf2dLHBAIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizing** (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).<br>\n",
        "**Adding new tokens** to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePieceâ€¦).<br>\n",
        "**Managing special tokens** (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization.\n"
      ],
      "metadata": {
        "id": "BIs53rIl4FKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "br6Dw5lVBGDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then pass your sentence to the tokenizer:"
      ],
      "metadata": {
        "id": "kXERRI3aBKm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples of tokens:<br>\n",
        ">101 - beginning of text<br>\n",
        "102 - end of text<br>\n",
        "119 - period\n",
        "Punctuation is included"
      ],
      "metadata": {
        "id": "mcuRi6dK4QTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input1 = tokenizer(\".I and am you are the\")\n",
        "print(encoded_input1)"
      ],
      "metadata": {
        "id": "BLZgTiUA23QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input2 = tokenizer(\"I am. You are not.\")\n",
        "print(encoded_input2)"
      ],
      "metadata": {
        "id": "itU4wG2L4W6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "YXGkWVA-BLK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer returns a dictionary with three important itmes:<br>\n",
        "\n",
        "- input_ids are the indices corresponding to each token in the sentence.<br>\n",
        "- attention_mask indicates whether a token should be attended to or not.<br>\n",
        "- token_type_ids identifies which sequence a token belongs to when there is more than one sequence.<br>\n",
        "\n",
        "You can decode the input_ids to return the original input:"
      ],
      "metadata": {
        "id": "PTffKIBoBQhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "fqP0oQy5BP2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the tokenizer added two special tokens - CLS and SEP (classifier and separator) - to the sentence. Not all models need special tokens, but if they do, the tokenizer will automatically add them for you.\n",
        "\n",
        "If there are several sentences you want to process, pass the sentences as a list to the tokenizer:"
      ],
      "metadata": {
        "id": "jWapsYxPBZIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "print(encoded_inputs)\n"
      ],
      "metadata": {
        "id": "ph8i84f7BZ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pad**<br>\n",
        "This brings us to an important topic. When you process a batch of sentences, they aren't always the same length. <br>\n",
        "**This is a problem because tensors, the input to the model, need to have a uniform shape.** <br>\n",
        "Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to sentences with fewer tokens.<br>\n",
        "**Set the padding parameter to True to pad the shorter sequences in the batch to match the longest sequence:**"
      ],
      "metadata": {
        "id": "E34drX5oBfWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True)\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "IoT8kyVFBgIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Truncation**<br>\n",
        "Sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length.\n",
        "\n",
        "**Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model:**"
      ],
      "metadata": {
        "id": "o4TsyJdGBy8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "usWUFfw7Bzx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build tensors**<br>\n",
        "Finally, you want the tokenizer to return the actual tensors that are fed to the model.\n",
        "\n",
        "Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow:\n",
        "\n"
      ],
      "metadata": {
        "id": "KTp3Mli8B3zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(encoded_input)\n"
      ],
      "metadata": {
        "id": "m-kDI0-qB4kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(encoded_input)\n"
      ],
      "metadata": {
        "id": "pFXxbYEJCEX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pad and truncate\n",
        "Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:\n",
        "\n"
      ],
      "metadata": {
        "id": "7pGZMWFVDTnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
      ],
      "metadata": {
        "id": "2uBJ_MUPJooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"audio\"]\n"
      ],
      "metadata": {
        "id": "NIzuNJRtJr3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"audio\"][\"array\"].shape\n"
      ],
      "metadata": {
        "id": "ICKadJcZDYKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][\"audio\"][\"array\"].shape\n"
      ],
      "metadata": {
        "id": "HnU5zU1BDbct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the first sample has a longer sequence than the second sample. Let's create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"
      ],
      "metadata": {
        "id": "QGIGxHWNDXRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "metadata": {
        "id": "p6jiVRR2KIuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        max_length=100000,\n",
        "        truncation=True,\n",
        "    )\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "bv4Aepg7De1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the function to the the first few examples in the dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "w3gy39shDiRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = preprocess_function(dataset[:5])"
      ],
      "metadata": {
        "id": "oatXFp9kDjNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now take another look at the processed sample lengths:"
      ],
      "metadata": {
        "id": "g7zbEs2GDoEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "processed_dataset[\"input_values\"][0].shape\n"
      ],
      "metadata": {
        "id": "T0m_5IOGDoqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset[\"input_values\"][1].shape\n"
      ],
      "metadata": {
        "id": "R3fhdYnADr-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lengths of the first two samples now match the maximum length you specified."
      ],
      "metadata": {
        "id": "4nlCDOJ7Dutr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision\n",
        "A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input.\n",
        "\n",
        "Let's load the food101 dataset for this tutorial. Use ðŸ¤— Datasets split parameter to only load a small sample from the training split since the dataset is quite large:"
      ],
      "metadata": {
        "id": "M8ocJiIBDvmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"food101\", split=\"train[:100]\")"
      ],
      "metadata": {
        "id": "uvO09mXGD9WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, take a look at the image with ðŸ¤— Datasets Image feature:"
      ],
      "metadata": {
        "id": "kt9mie2yGYz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"image\"]"
      ],
      "metadata": {
        "id": "RshMmDjTGZ3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use convert to change the image. "
      ],
      "metadata": {
        "id": "us7lmRIOmA0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img1=dataset[0][\"image\"].convert(mode=\"L\")\n",
        "img1"
      ],
      "metadata": {
        "id": "Be7ohkAdRdkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extractor\n",
        "Load the feature extractor with AutoFeatureExtractor.from_pretrained():"
      ],
      "metadata": {
        "id": "O6kuqEONGeP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")"
      ],
      "metadata": {
        "id": "mUrdmufxGfG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation\n",
        "For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you'd like, but in this tutorial, you will use torchvision's transforms module.\n",
        "\n",
        "Normalize the image and use Compose to chain some transforms - RandomResizedCrop and ColorJitter - together:"
      ],
      "metadata": {
        "id": "VttxJjZEGjrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor\n",
        "\n",
        "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "transformations_to_array = Compose([ToTensor(), normalize]\n",
        ")"
      ],
      "metadata": {
        "id": "gtZq3pJhGkRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model accepts pixel_values as it's input. This value is generated by the feature extractor. Create a function that generates pixel_values from the transforms:"
      ],
      "metadata": {
        "id": "y1L33hWcGojm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "wUquF7fXKk4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transforms(examples):\n",
        "    examples[\"pixel_values\"] = [transformations_to_array(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return examples"
      ],
      "metadata": {
        "id": "Jpg78umuGqgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr=transformations_to_array(dataset[0][\"image\"]) "
      ],
      "metadata": {
        "id": "aCtqjaCwcNHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then use ðŸ¤— Datasets set_transform to apply the transforms on-the-fly:"
      ],
      "metadata": {
        "id": "qyxsnE1JGtNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr"
      ],
      "metadata": {
        "id": "-7-iqjVUGt_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.set_transform(transformations_to_array)"
      ],
      "metadata": {
        "id": "RFRCTSNuVMG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now when you access the image, you will notice the feature extractor has added the model input pixel_values:"
      ],
      "metadata": {
        "id": "A_aFpdA1Gxm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what the image looks like after you preprocess it. Just as you'd expect from the applied transforms, the image has been randomly cropped and it's color properties are different."
      ],
      "metadata": {
        "id": "hiIXslQTKA4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(tr.permute(1, 2, 0))\n"
      ],
      "metadata": {
        "id": "6JnKgznmJ72j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][\"image\"]"
      ],
      "metadata": {
        "id": "eL534uQzpQtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms_to_resize = Compose(\n",
        "    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize])"
      ],
      "metadata": {
        "id": "tYEsVF-LngDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_rs=transformations_to_array(dataset[1][\"image\"]) "
      ],
      "metadata": {
        "id": "IKIWB9uJnvIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.imshow(tr_rs.permute(1, 2, 0))\n"
      ],
      "metadata": {
        "id": "gwhCqlqen2Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multimodal\n",
        "For multimodal tasks. you will use a combination of everything you've learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:\n",
        "\n",
        "Feature extractor to preprocess the audio data.\n",
        "Tokenizer to process the text.\n",
        "Let's return to the LJ Speech dataset:"
      ],
      "metadata": {
        "id": "bQkrOU74phgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "lj_speech = load_dataset(\"lj_speech\", split=\"train\")"
      ],
      "metadata": {
        "id": "GcHEiWZ0pk9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since you are mainly interested in the audio and text column, remove the other columns:"
      ],
      "metadata": {
        "id": "3gcD1JREpqmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lj_speech = lj_speech.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])"
      ],
      "metadata": {
        "id": "ZUjaK-u9prlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now take a look at the audio and text columns:\n",
        "\n"
      ],
      "metadata": {
        "id": "AHVGzEaXpu0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lj_speech[0][\"audio\"]"
      ],
      "metadata": {
        "id": "tWFwW-9zpxFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lj_speech[0][\"text\"]"
      ],
      "metadata": {
        "id": "h1GTpHCxpzHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember from the earlier section on processing audio data, you should always resample your audio data's sampling rate to match the sampling rate of the dataset used to pretrain a model:"
      ],
      "metadata": {
        "id": "W2hlUBo7p1Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lj_speech = lj_speech.cast_column(\"audio\", Audio(sampling_rate=16_000))"
      ],
      "metadata": {
        "id": "jn8jpk9Pp13i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processor\n",
        "A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"
      ],
      "metadata": {
        "id": "ifHOWcY9qn0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "id": "lPNrbRstqrNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to process the audio data to input_values, and tokenizes the text to labels. These are your inputs to the model:\n"
      ],
      "metadata": {
        "id": "XNDUiC-IqtO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example[\"input_values\"] = processor(audio[\"array\"], sampling_rate=16000)\n",
        "\n",
        "    with processor.as_target_processor():\n",
        "        example[\"labels\"] = processor(example[\"text\"]).input_ids\n",
        "    return example\n",
        "     "
      ],
      "metadata": {
        "id": "U8iTrz3-qvVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the prepare_dataset function to a sample:"
      ],
      "metadata": {
        "id": "H373dJCNqyo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_dataset(lj_speech[0])\n"
      ],
      "metadata": {
        "id": "l744XN_Uq1_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the processor has added input_values and labels. The sampling rate has also been correctly downsampled to 16kHz.\n",
        "\n",
        "Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."
      ],
      "metadata": {
        "id": "EGgYlPlYq32X"
      }
    }
  ]
}