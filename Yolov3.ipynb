{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolov3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMswWYWIsRPBRGpRGgZ6Ip8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/Yolov3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMtoEPHZN5cE"
      },
      "source": [
        "Need to upload yolov3.weights - it is too big for github\n",
        "\n",
        "Run all the code cells. <br>\n",
        "The go do something else for about ~13 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtBDYNMc9cAP"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAo4xBW9kqM"
      },
      "source": [
        "# **YOLOv3 Architecture**\n",
        "YOLO is not the most accurate algorithm. RetinaNet, and SSD outperform it in terms of accuracy. <br>\n",
        "\n",
        "It still, however, was one of the fastest.<br>\n",
        "\n",
        "But that speed has been traded off for boosts in accuracy in YOLO v3. While the earlier variant ran on 45 FPS on a Titan X, the current version clocks about 30 FPS. This has to do with the increase in complexity of underlying architecture called Darknet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrUpnCbV9ASg"
      },
      "source": [
        "# **Darknet-53**\n",
        "YOLO v3 uses a variant of Darknet, which originally had a 53 layer network trained on Imagenet. For the task of detection, 53 more layers are stacked onto it, giving us a **106 layer fully convolutional underlying architecture for YOLO v3**. <br>\n",
        "This is the reason behind the slowness of YOLO v3 compared to YOLO v2.\n",
        "\n",
        "*The most salient feature of v3 is that it makes detections at three different scales.*\n",
        "\n",
        "Which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively.\n",
        "\n",
        "The 13 x 13 layer is responsible for detecting large objects, whereas the 52 x 52 layer detects the smaller objects, with the 26 x 26 layer detecting medium objects.\n",
        "\n",
        "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR4QJalP9Fat"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"/content/YOLO3architecture.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmc3KYdq9R-p"
      },
      "source": [
        "Underneath it uses YOLOv3 model trained on [COCO dataset](https://cocodataset.org/#home) capable of detecting 80 common objects in context.\n",
        "The common objects are:<br>\n",
        "person,\n",
        "bicycle,\n",
        "car,\n",
        "motorcycle,\n",
        "airplane,\n",
        "bus,\n",
        "train,\n",
        "truck,\n",
        "boat,\n",
        "traffic light,\n",
        "fire hydrant,\n",
        "stop sign,\n",
        "parking meter,\n",
        "bench,\n",
        "bird,\n",
        "cat,\n",
        "dog,\n",
        "horse,\n",
        "sheep,\n",
        "cow,\n",
        "elephant,\n",
        "bear,\n",
        "zebra,\n",
        "giraffe,\n",
        "backpack,\n",
        "umbrella,\n",
        "handbag,\n",
        "tie,\n",
        "suitcase,\n",
        "frisbee,\n",
        "skis,\n",
        "snowboard,\n",
        "sports ball,\n",
        "kite,\n",
        "baseball bat,\n",
        "baseball glove,\n",
        "skateboard,\n",
        "surfboard,\n",
        "tennis racket,\n",
        "bottle,\n",
        "wine glass,\n",
        "cup,\n",
        "fork,\n",
        "knife,\n",
        "spoon,\n",
        "bowl,\n",
        "banana,\n",
        "apple,\n",
        "sandwich,\n",
        "orange,\n",
        "broccoli,\n",
        "carrot,\n",
        "hot dog,\n",
        "pizza,\n",
        "donut,\n",
        "cake,\n",
        "chair,\n",
        "couch,\n",
        "potted plant,\n",
        "bed,\n",
        "dining table,\n",
        "toilet,\n",
        "tv,\n",
        "laptop,\n",
        "mouse,\n",
        "remote,\n",
        "keyboard,\n",
        "cell phone,\n",
        "microwave,\n",
        "oven,\n",
        "toaster,\n",
        "sink,\n",
        "refrigerator,\n",
        "book,\n",
        "clock,\n",
        "vase,\n",
        "scissors,\n",
        "teddy bear,\n",
        "hair dryer,\n",
        "toothbrush"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7B9cwO3qHj_"
      },
      "source": [
        "# load the COCO class labels our YOLO model was trained on\n",
        "LABELS = open(\"yolo-object-detection/yolo-coco/coco.names\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUjWiz9BObr"
      },
      "source": [
        "print(LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GeM_WTEp9y1"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gloBMgmJBSe5"
      },
      "source": [
        "**Get the pretrained YOLOv3 weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1SdkpYQPwmd"
      },
      "source": [
        "! wget \"https://pjreddie.com/media/files/yolov3.weights\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PJx7Eu2Be6N"
      },
      "source": [
        "**Each label is assigned a color**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFvzQFbIqnBe"
      },
      "source": [
        "# initialize a list of colors to represent each possible class label\n",
        "np.random.seed(42)\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
        "\tdtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmXT8nL3Bl_G"
      },
      "source": [
        "**Load the Darknet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjGu2Btpqs0j"
      },
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = \"/content/cloned-repo/cloned-repo/yolov3.weights\"\n",
        "configPath = \"/content/cloned-repo/cloned-repo/yolo-object-detection/yolo-coco/yolov3.cfg\"\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "print(\"[INFO] loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5O7MS4wBwcY"
      },
      "source": [
        "**Load an image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9KaRJkFBzKu"
      },
      "source": [
        "# load our input image and grab its spatial dimensions\n",
        "image = cv2.imread(\"/content/cloned-repo/yolo-object-detection/images/baggage_claim.jpg\")\n",
        "(H, W) = image.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxpAbppqxXs"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "# construct a blob from the input image and then perform a forward\n",
        "# pass of the YOLO object detector, giving us our bounding boxes and\n",
        "# associated probabilities\n",
        "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
        "\tswapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "layerOutputs = net.forward(ln)\n",
        "end = time.time()\n",
        "# show timing information on YOLO\n",
        "print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiO4mVk2B8Q3"
      },
      "source": [
        "**Set up the lists for the bounding boxes, confidences, and classIDs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOlJZWDyq0ea"
      },
      "source": [
        "# initialize our lists of detected bounding boxes, confidences, and\n",
        "# class IDs, respectively\n",
        "boxes = []\n",
        "confidences = []\n",
        "classIDs = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewf6KMBq5IG"
      },
      "source": [
        "# loop over each of the layer outputs\n",
        "for output in layerOutputs:\n",
        "\t# loop over each of the detections\n",
        "\tfor detection in output:\n",
        "\t\t# extract the class ID and confidence (i.e., probability) of\n",
        "\t\t# the current object detection\n",
        "\t\tscores = detection[5:]\n",
        "\t\tclassID = np.argmax(scores)\n",
        "\t\tconfidence = scores[classID]\n",
        "\t\t# filter out weak predictions by ensuring the detected\n",
        "\t\t# probability is greater than the minimum probability\n",
        "\t\tif confidence > .50:\n",
        "\t\t\t# scale the bounding box coordinates back relative to the\n",
        "\t\t\t# size of the image, keeping in mind that YOLO actually\n",
        "\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
        "\t\t\t# box followed by the boxes' width and height\n",
        "\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
        "\t\t\t# and left corner of the bounding box\n",
        "\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\ty = int(centerY - (height / 2))\n",
        "\t\t\t# update our list of bounding box coordinates, confidences,\n",
        "\t\t\t# and class IDs\n",
        "\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\tconfidences.append(float(confidence))\n",
        "\t\t\tclassIDs.append(classID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYH941qmCKuB"
      },
      "source": [
        "print(boxes)\n",
        "print(confidences)\n",
        "print(classIDs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MH_DPsKq7B0"
      },
      "source": [
        "# apply non-maxima suppression to suppress weak, overlapping bounding\n",
        "# boxes\n",
        "idxs = cv2.dnn.NMSBoxes(boxes, confidences, .50,0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzMvrkc-rASz"
      },
      "source": [
        "# ensure at least one detection exists\n",
        "if len(idxs) > 0:\n",
        "\t# loop over the indexes we are keeping\n",
        "\tfor i in idxs.flatten():\n",
        "\t\t# extract the bounding box coordinates\n",
        "\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\t\t# draw a bounding box rectangle and label on the image\n",
        "\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
        "\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t\t0.5, color, 2)\n",
        "# show the output image\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChS7pQpLEn_p"
      },
      "source": [
        "image = cv2.imread(\"/content/cloned-repo/yolo-object-detection/images/baggage_claim.jpg\")\n",
        "(H, W) = image.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GklEaOE0po"
      },
      "source": [
        "!pip3 install opencv-python tensorflow\n",
        "!pip3 install drawbox\n",
        "!pip install --upgrade cvlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QinG61P0Ewh6"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import cvlib as cv\n",
        "from cvlib.object_detection import draw_bbox\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAXwPVJGEf8w"
      },
      "source": [
        "bbox, label, conf = cv.detect_common_objects(image)\n",
        "output_image2 = draw_bbox(image, bbox, label, conf)\n",
        "plt.imshow(output_image2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnbGnEGoIruv"
      },
      "source": [
        "import pandas as pd\n",
        "ds1 = pd.Series(bbox)\n",
        "ds2 = pd.Series(conf)\n",
        "ds3 = pd.Series(label)\n",
        "dframe={'boxes':ds1,'confidences': ds2,'classIDs': ds3}\n",
        "df=pd.DataFrame(dframe)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}