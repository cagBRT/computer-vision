{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV4 Advanced ContentBasedImageRetrieval.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJpYG7Dj4EM12zMFUdPpUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV4_Advanced_ContentBasedImageRetrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfVbNuvXD4UB"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKmVCXvEi9iQ"
      },
      "source": [
        "# **Content Based Image Retrieval (CBIR)** \n",
        "CBIR uses an image to search for similiar images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc3uvI886Njw"
      },
      "source": [
        "This notebook uses the MNIST training dataset to train the autoencoder. <br>\n",
        "Then use the MNIST test dataset to make predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIspgZevcMUE"
      },
      "source": [
        "**The CBIR Steps**:<br>\n",
        ">Phase #1: Train the autoencoder<br>\n",
        "Phase #2: Extract features from all images in our dataset by computing their latent-space representations using the autoencoder<br>\n",
        "Phase #3: Compare latent-space vectors to find all relevant images in the dataset<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn1FBmiQubJn"
      },
      "source": [
        "https://www.pyimagesearch.com/2020/03/30/autoencoders-for-content-based-image-retrieval-with-keras-and-tensorflow/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2SG7SZicEva"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoNGguyec7Cr"
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "# import the necessary packages\n",
        "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuv5ziDwbxiB"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVLxLNuxsewu"
      },
      "source": [
        "# import the necessary packages\n",
        "from imutils import build_montages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKX6DzzQyWpo"
      },
      "source": [
        "# **Autoencoders**\n",
        "Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.<br>\n",
        "<br>\n",
        "Autoencoders, by design, reduce data dimensions by learning how to ignore the noise in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PRLPwguy3l1"
      },
      "source": [
        "image = cv2.imread(\"images/autoencoder.jpeg\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKWNxIyHzFWA"
      },
      "source": [
        "# **Autoencoders consist of four main parts:<br>**\n",
        "1- **Encoder**: the model learns to reduce the input dimensions and compress the input data into an encoded representation.<br>\n",
        "2- **Bottleneck**: the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.<br>\n",
        "3- **Decoder**: the model learns how to reconstruct the data from the encoded representation, which should be as close to the original input as possible.<br>\n",
        "4- **Reconstruction Loss**: This is the method that measures how well the decoder is performing and how close the output is to the original input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nar63a2a0hSM"
      },
      "source": [
        "# **Deep learning-based CBIR and image retrieval**\n",
        "\n",
        "When training the autoencoder, we do not use class labels, which can be considered a form of unsupervised learning.<br>\n",
        "The autoencoder computes the latent-space vector representation for each image in our dataset (i.e., our “feature vector” for a given image)\n",
        "Then, at search time, we compute the distance between the latent-space vectors — the smaller the distance, the more relevant/visually similar are the two images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAJ4JAIw1eA6"
      },
      "source": [
        "image = cv2.imread(\"images/keras_autoencoder_steps.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1jOjVy_3wQJ"
      },
      "source": [
        "**Latent Space Projector**<br>\n",
        "http://projector.tensorflow.org/\n",
        "\n",
        "Go to this link and search for a word. <br>\n",
        "You will see all the words close to your word. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8EqFfbz4jrg"
      },
      "source": [
        "image = cv2.imread(\"images/latentSpace.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5kgvF5V52ix"
      },
      "source": [
        "# **Build the Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Vhbe1tcyVE"
      },
      "source": [
        "class ConvAutoencoder:\n",
        "\t@staticmethod\n",
        "\t#use this class to create an Autoencoder of any size and shape\n",
        "\n",
        "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
        "\t\t# initialize the input shape to be \"channels last\" along with\n",
        "\t\t# the channels dimension itself\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t#1. define the input to the encoder\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = inputs\n",
        "\n",
        "\t\t#2. loop over the number of filters\n",
        "\t\tfor f in filters:\n",
        "\t\t\t# apply a CONV => RELU => BN operation\n",
        "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t \n",
        "\t\t#3. flatten the network and then construct our latent vector\n",
        "\t\tvolumeSize = K.int_shape(x)\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tlatent = Dense(latentDim, name=\"encoded\")(x)\n",
        "\t\t#this layer is named because we will access this layer later\n",
        "\t\n",
        "\t\t#===Building the decoder===#\n",
        "\t\t#4. start building the decoder model which will accept the\n",
        "\t\t# output of the encoder as its inputs\n",
        "\t\tx = Dense(np.prod(volumeSize[1:]))(latent)\n",
        "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
        "\t\n",
        "\t\t#5. loop over our number of filters again, but this time in\n",
        "\t\t# reverse order\n",
        "\t\tfor f in filters[::-1]:\n",
        "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
        "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
        "\t\t\t\tpadding=\"same\")(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
        "\t \n",
        "\t\t#6. apply a single CONV_TRANSPOSE layer used to recover the\n",
        "\t\t# original depth of the image\n",
        "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
        "\t\toutputs = Activation(\"sigmoid\", name=\"decoded\")(x)\n",
        "\t\n",
        "\t\t#7. construct our autoencoder model\n",
        "\t\tautoencoder = Model(inputs, outputs, name=\"autoencoder\")\n",
        "\t\n",
        "\t\t# return the autoencoder model\n",
        "\t\treturn autoencoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VZ89_2j5wjs"
      },
      "source": [
        "# **Visualize the predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGVSIIhLc7FQ"
      },
      "source": [
        "def visualize_predictions(decoded, gt, samples=30):\n",
        "\t# initialize our list of output images\n",
        "\toutputs = None\n",
        "\t# loop over our number of output samples\n",
        "\tfor i in range(0, samples):\n",
        "\t\t# grab the original image and reconstructed image\n",
        "\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n",
        "\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
        "\t\t# stack the original and reconstructed image side-by-side\n",
        "\t\toutput = np.hstack([original, recon])\n",
        "\t\t# if the outputs array is empty, initialize it as the current\n",
        "\t\t# side-by-side image display\n",
        "\t\tif outputs is None:\n",
        "\t\t\toutputs = output\n",
        "\t\t# otherwise, vertically stack the outputs\n",
        "\t\telse:\n",
        "\t\t\toutputs = np.vstack([outputs, output])\n",
        "\t# return the output images\n",
        "\treturn outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWvK89Sk6HNJ"
      },
      "source": [
        "# **Train the autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDPLxEhG31CW"
      },
      "source": [
        "# initialize the number of epochs to train for, initial learning rate,\n",
        "# and batch size\n",
        "EPOCHS = 20\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "# load the MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "# construct our convolutional autoencoder\n",
        "print(\"[INFO] building autoencoder...\")\n",
        "autoencoder = ConvAutoencoder.build(28, 28, 1)\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
        "# train the convolutional autoencoder\n",
        "H = autoencoder.fit(\n",
        "\ttrainX, trainX,\n",
        "\tvalidation_data=(testX, testX),\n",
        "\tepochs=EPOCHS,\n",
        "\tbatch_size=BS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evkDU4nC7R4F"
      },
      "source": [
        "# **Make predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvxXMULQ4Igv"
      },
      "source": [
        "# use the convolutional autoencoder to make predictions on the\n",
        "# testing images, construct the visualization, and then save it\n",
        "# to disk\n",
        "print(\" making predictions...\")\n",
        "print(\"original on the left, reconstructed on the right\")\n",
        "decoded = autoencoder.predict(testX)\n",
        "vis = visualize_predictions(decoded, testX)\n",
        "#cv2.imwrite('imageTest.jpg', vis)\n",
        "cv2_imshow(vis)\n",
        "\n",
        "# serialize the autoencoder model to disk\n",
        "#print(\"[INFO] saving autoencoder...\")\n",
        "autoencoder.save('H', save_format=\"h5\")\n",
        "#save the model as \"modelOut\"\n",
        "autoencoder.save(\"modelOut\", save_format=\"h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Tsm0Uf8A_9"
      },
      "source": [
        "# **Plot the training loss and accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fbLnqMPIL9-"
      },
      "source": [
        "# construct a plot that plots and saves the training history\n",
        "N = np.arange(0, EPOCHS)\n",
        "#plt.style.use(\"ggplot\")\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "#plt.savefig(\"plot.jpg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYNUS2NRbu2Q"
      },
      "source": [
        "The autoencoder is now trained. <br>\n",
        "Now we do the feature extraction and indexing stage of the image retrieval pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIlFjVi7b62O"
      },
      "source": [
        "# load the MNIST dataset\n",
        "print(\"loading MNIST training split...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "\n",
        "# add a channel dimension to every image in the training split, then\n",
        "# normalize the data of each pixel\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDW4E-YAb9oM"
      },
      "source": [
        "# load the autoencoder we created and called modelOut\n",
        "print(\"loading the autoencoder model...\")\n",
        "autoencoder = load_model(\"modelOut\")\n",
        "# create the encoder model which consists of *just* the encoder\n",
        "# portion of the autoencoder\n",
        "encoder = Model(inputs=autoencoder.input,\n",
        "\toutputs=autoencoder.get_layer(\"encoded\").output)\n",
        "# quantify the contents of our input images using the encoder\n",
        "print(\"encoding images...\")\n",
        "features = encoder.predict(trainX)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbC863QfcCn1"
      },
      "source": [
        "# construct a dictionary that maps the index of the MNIST training\n",
        "# image to its corresponding latent-space representation\n",
        "indexes = list(range(0, trainX.shape[0]))\n",
        "data = {\"indexes\": indexes, \"features\": features}\n",
        "# write the data dictionary to disk\n",
        "print(\"saving index...\")\n",
        "f = open(\"index.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(data))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXC2s3RV_Nud"
      },
      "source": [
        "**Dictionaries**<br>\n",
        "Dictionaries are Python's implementation of a data structure that is more generally known as an associative array. A dictionary consists of a collection of key-value pairs. Each key-value pair maps the key to its associated value.<br>\n",
        "Our dictionary has: <br>\n",
        ">indexes: Integer indices of each MNIST digit image in the dataset<br>\n",
        "features: The corresponding feature vector for each image in the dataset<br>\n",
        "\n",
        "It is in the file called index.pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GyBQDty-qgQ"
      },
      "source": [
        "#print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLRBGy73AO8t"
      },
      "source": [
        "Define a function called euclidean<br>\n",
        "This function calculates the similarity between two feature vectors. <br>\n",
        "It uses euclidean distance - cosine distance in this case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjJ3xOh9shur"
      },
      "source": [
        "def euclidean(a, b):\n",
        "\t# compute and return the euclidean distance between two vectors\n",
        "\treturn np.linalg.norm(a - b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8dyykZfAyFa"
      },
      "source": [
        "# **Define the search function**<br>\n",
        "This function is responsible for comparing all feature vectors for similarity and returning the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hKF3t7xslf1"
      },
      "source": [
        "def perform_search(queryFeatures, index, maxResults=64):\n",
        "\t# initialize our list of results\n",
        "\tresults = []\n",
        "\n",
        "\t# loop over our index\n",
        "\tfor i in range(0, len(index[\"features\"])):\n",
        "\t\t# compute the euclidean distance between our query features\n",
        "\t\t# and the features for the current image in our index, then\n",
        "\t\t# update our results list with a 2-tuple consisting of the\n",
        "\t\t# computed distance and the index of the image\n",
        "\t\td = euclidean(queryFeatures, index[\"features\"][i])\n",
        "\t\tresults.append((d, i))\n",
        "  \n",
        "\t# sort the results and grab the top ones\n",
        "\tresults = sorted(results)[:maxResults]\n",
        "\t# return the list of results\n",
        "\treturn results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG49oEZpA9dj"
      },
      "source": [
        "# **Load and preprocess the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUnCqJ52sxRM"
      },
      "source": [
        "# load the MNIST dataset\n",
        "print(\"loading MNIST dataset...\")\n",
        "((trainX, _), (testX, _)) = mnist.load_data()\n",
        "# add a channel dimension to every image in the dataset, then scale\n",
        "# the pixel intensities to the range [0, 1]\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk8e-vPoBylu"
      },
      "source": [
        "**Load the autoencoder and index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKnVlEcNs1A4"
      },
      "source": [
        "# load the autoencoder model and index from disk\n",
        "print(\"loading autoencoder and index...\")\n",
        "autoencoder = load_model(\"modelOut\")\n",
        "index = pickle.loads(open(\"index.pickle\", \"rb\").read())\n",
        "# create the encoder model which consists of *just* the encoder\n",
        "# portion of the autoencoder\n",
        "encoder = Model(inputs=autoencoder.input,\n",
        "\toutputs=autoencoder.get_layer(\"encoded\").output)\n",
        "# quantify the contents of our input testing images using the encoder\n",
        "print(\"[INFO] encoding testing images...\")\n",
        "features = encoder.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNfbfS3wCAVA"
      },
      "source": [
        "**Use a random sample of the set as queries**<br>\n",
        "10 random digits are selected and used as queries to the dataset. <br>\n",
        "225 closest results are returned.<br>\n",
        "Check the results - which searches were the most/least successful?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6yKOuAtJCS"
      },
      "source": [
        "# randomly sample a set of testing query image indexes\n",
        "queryIdxs = list(range(0, testX.shape[0]))\n",
        "queryIdxs = np.random.choice(queryIdxs, size=10,\n",
        "\treplace=False)\n",
        "# loop over the testing indexes\n",
        "for i in queryIdxs:\n",
        "\t# take the features for the current image, find all similar\n",
        "\t# images in our dataset, and then initialize our list of result\n",
        "\t# images\n",
        "\tqueryFeatures = features[i]\n",
        "\tresults = perform_search(queryFeatures, index, maxResults=225)\n",
        "\timages = []\n",
        "\t# loop over the results\n",
        "\tfor (d, j) in results:\n",
        "\t\t# grab the result image, convert it back to the range\n",
        "\t\t# [0, 255], and then update the images list\n",
        "\t\timage = (trainX[j] * 255).astype(\"uint8\")\n",
        "\t\timage = np.dstack([image] * 3)\n",
        "\t\timages.append(image)\n",
        "\t# display the query image\n",
        "\tquery = (testX[i] * 255).astype(\"uint8\")\n",
        "\tcv2_imshow( query)\n",
        "\t# build a montage from the results and display it\n",
        "\tmontage = build_montages(images, (28, 28), (15, 15))[0]\n",
        "\tcv2_imshow(montage)\n",
        "\tcv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM7ryefx2QGR"
      },
      "source": [
        "https://www.pyimagesearch.com/2020/03/30/autoencoders-for-content-based-image-retrieval-with-keras-and-tensorflow/\n"
      ]
    }
  ]
}