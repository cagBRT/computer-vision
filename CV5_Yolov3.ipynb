{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV5 Yolov3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNTkBeJ7f8ncz4kPyeYOIQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV5_Yolov3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMtoEPHZN5cE"
      },
      "source": [
        "**Need to upload yolov3.weights - it is too big for github**<br>\n",
        "The upload could take as long as 13 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1SdkpYQPwmd"
      },
      "source": [
        "! wget \"https://pjreddie.com/media/files/yolov3.weights\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtBDYNMc9cAP"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAo4xBW9kqM"
      },
      "source": [
        "# **YOLOv3 Architecture**\n",
        "YOLO is not the most accurate algorithm. RetinaNet, and SSD outperform it in terms of accuracy. <br>\n",
        "\n",
        "It still, however, was one of the fastest.<br>\n",
        "\n",
        "But that speed has been traded off for boosts in accuracy in YOLO v3. While the earlier variant ran on 45 FPS on a Titan X, the current version clocks about 30 FPS. This has to do with the increase in complexity of underlying architecture called Darknet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQWt0Fq3LruC"
      },
      "source": [
        "YOLO looks at the image once, instead of many times. The image is divided into a grid. Each cell in the grid can predict 2 bounding boxes. <br>\n",
        "The bounding boxes are drawn for each object detected (upper center image). This can result in many bounding boxes. The heavier the box lines the higher the confidence in the prediction. <br>\n",
        "The highest confidence boxes that have little overlap are kept. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjRkeXnnLgoB"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"images/YOLOGrids.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrUpnCbV9ASg"
      },
      "source": [
        "# **Darknet-53**\n",
        "YOLO v3 uses a variant of Darknet, which originally had a 53 layer network trained on Imagenet. For the task of detection, 53 more layers are stacked onto it, giving us a **106 layer fully convolutional underlying architecture for YOLO v3**. <br>\n",
        "This is the reason behind the slowness of YOLO v3 compared to YOLO v2.\n",
        "\n",
        "*The most salient feature of v3 is that it makes detections at three different scales.*\n",
        "\n",
        "Which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively.\n",
        "\n",
        "The 13 x 13 layer is responsible for detecting large objects, whereas the 52 x 52 layer detects the smaller objects, with the 26 x 26 layer detecting medium objects.\n",
        "\n",
        "https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR4QJalP9Fat"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"images/YOLO3architecture.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6iXSjLNXsfh"
      },
      "source": [
        "**YOLOv3 does not use Softmax**<br>\n",
        "YOLO v3 performs multilabel classification for objects detected in images.\n",
        "Earlier in YOLO, authors used to softmax the class scores and take the class with maximum score to be the class of the object contained in the bounding box. <br>\n",
        "\n",
        "Softmaxing classes assumes classes are mutually exclusive, or in simple words, if an object belongs to one class, then it cannot belong to the other. This works fine in COCO dataset.<br>\n",
        "However, when we have classes like Person and Women in a dataset, then the  assumption fails. <br>\n",
        "\n",
        "This is the reason why the authors of YOLO have refrained from softmaxing the classes. Instead, each class score is predicted using logistic regression and a threshold is used to predict multiple labels for an object. Classes with scores higher than this threshold are assigned to the box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXgEoLrEQH43"
      },
      "source": [
        "The object's centroid is in the red grid, which means it is responsible for detecting the dog. <br>\n",
        "The feature vector lists:<BR>\n",
        "- the coordinates of the bounding box\n",
        "- the confidence score of the object\n",
        "- the class scores for all the classes in the set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suDCjOdmP-lb"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"images/YOLOOutputVector.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQnMeLg-SukV"
      },
      "source": [
        "The simplified architecture is:\n",
        "- image input\n",
        "- image is overlaid with a grid\n",
        "- each cell produces the coordinates, the confidence score, and the class probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaaJQbbTSkQ1"
      },
      "source": [
        "Image(\"images/outputVector.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O3tQFulM3Hh"
      },
      "source": [
        "**Anchor boxes**<br>\n",
        "Both Faster R-CNN and YOLO use anchor boxes to improve object detection. <br>\n",
        "Anchor boxes are pre-defined bounding boxes with useful shapes and sizes that are tailored during training. <br>\n",
        "Anchor boxes shape and size is determined by the training data. Using a K-Means clustering algorithm, the training set is clustered by the size and location of the objects in the training images. These clusters determine the box size an shape. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3qxm4rqVARx"
      },
      "source": [
        "Image(\"images/KMeansBoxes.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn5WsQSpVVys"
      },
      "source": [
        "The anchor boxes are then used to detect objects in the image. <br>\n",
        "In the example below the two dotted black boxes are anchor boxes for the cell. The yellow and blue boxes are the output boxes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5y4p9B-UH21"
      },
      "source": [
        "Image(\"images/anchorBoxes.png\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmc3KYdq9R-p"
      },
      "source": [
        "Underneath it uses YOLOv3 model trained on [COCO dataset](https://cocodataset.org/#home) capable of detecting 80 common objects in context.\n",
        "The common objects are:<br>\n",
        "person,\n",
        "bicycle,\n",
        "car,\n",
        "motorcycle,\n",
        "airplane,\n",
        "bus,\n",
        "train,\n",
        "truck,\n",
        "boat,\n",
        "traffic light,\n",
        "fire hydrant,\n",
        "stop sign,\n",
        "parking meter,\n",
        "bench,\n",
        "bird,\n",
        "cat,\n",
        "dog,\n",
        "horse,\n",
        "sheep,\n",
        "cow,\n",
        "elephant,\n",
        "bear,\n",
        "zebra,\n",
        "giraffe,\n",
        "backpack,\n",
        "umbrella,\n",
        "handbag,\n",
        "tie,\n",
        "suitcase,\n",
        "frisbee,\n",
        "skis,\n",
        "snowboard,\n",
        "sports ball,\n",
        "kite,\n",
        "baseball bat,\n",
        "baseball glove,\n",
        "skateboard,\n",
        "surfboard,\n",
        "tennis racket,\n",
        "bottle,\n",
        "wine glass,\n",
        "cup,\n",
        "fork,\n",
        "knife,\n",
        "spoon,\n",
        "bowl,\n",
        "banana,\n",
        "apple,\n",
        "sandwich,\n",
        "orange,\n",
        "broccoli,\n",
        "carrot,\n",
        "hot dog,\n",
        "pizza,\n",
        "donut,\n",
        "cake,\n",
        "chair,\n",
        "couch,\n",
        "potted plant,\n",
        "bed,\n",
        "dining table,\n",
        "toilet,\n",
        "tv,\n",
        "laptop,\n",
        "mouse,\n",
        "remote,\n",
        "keyboard,\n",
        "cell phone,\n",
        "microwave,\n",
        "oven,\n",
        "toaster,\n",
        "sink,\n",
        "refrigerator,\n",
        "book,\n",
        "clock,\n",
        "vase,\n",
        "scissors,\n",
        "teddy bear,\n",
        "hair dryer,\n",
        "toothbrush"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7B9cwO3qHj_"
      },
      "source": [
        "# load the COCO class labels our YOLO model was trained on\n",
        "LABELS = open(\"yolo-object-detection/yolo-coco/coco.names\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUjWiz9BObr"
      },
      "source": [
        "#print(LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzjFMNx60Xiz"
      },
      "source": [
        "!pip3 install opencv-python\n",
        "!pip3 install tensorflow-object-detection-api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GeM_WTEp9y1"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "#import cvlib as cv\n",
        "#from cvlib.object_detection import draw_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gloBMgmJBSe5"
      },
      "source": [
        "**Get the pretrained YOLOv3 weights**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PJx7Eu2Be6N"
      },
      "source": [
        "**Each label is assigned a color**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFvzQFbIqnBe"
      },
      "source": [
        "# initialize a list of colors to represent each possible class label\n",
        "np.random.seed(42)\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
        "\tdtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmXT8nL3Bl_G"
      },
      "source": [
        "**Load the pretrained Darknet model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjGu2Btpqs0j"
      },
      "source": [
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = \"/content/cloned-repo/yolov3.weights\"\n",
        "configPath = \"/content/cloned-repo/yolo-object-detection/yolo-coco/yolov3.cfg\"\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "print(\"loading YOLO from disk...\")\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5O7MS4wBwcY"
      },
      "source": [
        "**Load an image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9KaRJkFBzKu"
      },
      "source": [
        "# load our input image and grab its spatial dimensions\n",
        "image = cv2.imread(\"/content/cloned-repo/yolo-object-detection/images/baggage_claim.jpg\")\n",
        "(H, W) = image.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYTQptmIxNtm"
      },
      "source": [
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzT2LehKX9g5"
      },
      "source": [
        "**Use the pretrained model to find objects in the image**<br>\n",
        "The model will find create a bounding box for each object it finds. In fact it may find the same object several times, resulting in multiple bounding boxes around the object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxpAbppqxXs"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "# construct a blob from the input image and then perform a forward\n",
        "# pass of the YOLO object detector, giving us our bounding boxes and\n",
        "# associated probabilities\n",
        "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
        "\tswapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "layerOutputs = net.forward(ln)\n",
        "end = time.time()\n",
        "# show timing information on YOLO\n",
        "print(\"YOLO took {:.6f} seconds\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiO4mVk2B8Q3"
      },
      "source": [
        "**Set up the lists for the bounding boxes, confidences, and classIDs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOlJZWDyq0ea"
      },
      "source": [
        "# initialize our lists of detected bounding boxes, confidences, and\n",
        "# class IDs, respectively\n",
        "boxes = []\n",
        "confidences = []\n",
        "classIDs = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhhUXop3lNgc"
      },
      "source": [
        "**Create the lists of boxes, confidences, and class IDs** <br>\n",
        "These are the bounding boxes that will be added to the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewf6KMBq5IG"
      },
      "source": [
        "# loop over each of the layer outputs\n",
        "for output in layerOutputs:\n",
        "\t# loop over each of the detections\n",
        "\tfor detection in output:\n",
        "\t\t# extract the class ID and confidence (i.e., probability) of\n",
        "\t\t# the current object detection\n",
        "\t\tscores = detection[5:]\n",
        "\t\tclassID = np.argmax(scores)\n",
        "\t\tconfidence = scores[classID]\n",
        "\t\t# filter out weak predictions by ensuring the detected\n",
        "\t\t# probability is greater than the minimum probability\n",
        "\t\t#====changed this confidence level=====\n",
        "\t\tif confidence > 0.10:\n",
        "\t\t\t# scale the bounding box coordinates back relative to the\n",
        "\t\t\t# size of the image, keeping in mind that YOLO actually\n",
        "\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
        "\t\t\t# box followed by the boxes' width and height\n",
        "\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
        "\t\t\t# and left corner of the bounding box\n",
        "\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\ty = int(centerY - (height / 2))\n",
        "\t\t\t# update our list of bounding box coordinates, confidences,\n",
        "\t\t\t# and class IDs\n",
        "\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\tconfidences.append(float(confidence))\n",
        "\t\t\tclassIDs.append(classID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYH941qmCKuB"
      },
      "source": [
        "print(boxes)\n",
        "print(confidences)\n",
        "print(classIDs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5heN7Lb4grWA"
      },
      "source": [
        "# apply non-maxima suppression to suppress weak, overlapping bounding\n",
        "# boxes\n",
        "idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAGtyJ-alncP"
      },
      "source": [
        "**Add the bounding boxes to the image**<br>\n",
        "Draw the bounding boxes on the image, and the confidence values and add the class ID. <br>\n",
        "Right now we are keeping every bounding box the model added. As you can see when you draw the image, some of the objects have more than one bounding box around them. This is because the model is detecting the object on different levels. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wg5PIBobeav"
      },
      "source": [
        "# ensure at least one detection exists\n",
        "if len(idxs) > 0:\n",
        "\t# loop over the indexes we are keeping\n",
        "\tfor i in idxs.flatten():\n",
        "\t\t# extract the bounding box coordinates\n",
        "\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\t\t# draw a bounding box rectangle and label on the image\n",
        "\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
        "\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t\t0.5, color, 2)\n",
        "# show the output image\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO_hdp8ZRzuG"
      },
      "source": [
        "In the image below the black box is the ground truth box. The red box is the predicted box. <br>\n",
        "- If no object exists in the cell, the conﬁdence scores should be zero. <br>\n",
        "- The conﬁdence score is equal to the intersection over union (IOU) between the predicted box and the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJZtxNMHRZbY"
      },
      "source": [
        "Image(\"images/confidence.jpeg\", width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjYRU__sYnJA"
      },
      "source": [
        "**Keep only the boxes with the highest confidence**<br>\n",
        "The model has detected a lot of objects in the image and created a bounding box, a score, and a confidence score for each one. <br>\n",
        "The model detected some objects more than once. <br>\n",
        "Let's have the model detect objects again, but this time we'll keep only those boxes that don't overlap and that have a higher confidence level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siHSEJL-dANh"
      },
      "source": [
        "# load our input image and grab its spatial dimensions\n",
        "image = cv2.imread(\"/content/cloned-repo/yolo-object-detection/images/baggage_claim.jpg\")\n",
        "(H, W) = image.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcAnPLqcnbfz"
      },
      "source": [
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwIz_klRnkV4"
      },
      "source": [
        "**Use the YOLO model to detect objects in the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTfKB26c0oi"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "# construct a blob from the input image and then perform a forward\n",
        "# pass of the YOLO object detector, giving us our bounding boxes and\n",
        "# associated probabilities\n",
        "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
        "\tswapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "start = time.time()\n",
        "layerOutputs = net.forward(ln)\n",
        "end = time.time()\n",
        "# show timing information on YOLO\n",
        "print(\"YOLO took {:.6f} seconds\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bSxJ8TeoWy0"
      },
      "source": [
        "**Compile the lists of boxes, confidences, and class IDs**.<br>\n",
        "But this time, only keep the boxes that have a confidence score higher than 50%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muq-M_3sa-qO"
      },
      "source": [
        "# loop over each of the layer outputs\n",
        "for output in layerOutputs:\n",
        "\t# loop over each of the detections\n",
        "\tfor detection in output:\n",
        "\t\t# extract the class ID and confidence (i.e., probability) of\n",
        "\t\t# the current object detection\n",
        "\t\tscores = detection[5:]\n",
        "\t\tclassID = np.argmax(scores)\n",
        "\t\tconfidence = scores[classID]\n",
        "\t\t# filter out weak predictions by ensuring the detected\n",
        "\t\t# probability is greater than the minimum probability\n",
        "\t\t#====changed this confidence level=====\n",
        "\t\tif confidence > .50:\n",
        "\t\t\t# scale the bounding box coordinates back relative to the\n",
        "\t\t\t# size of the image, keeping in mind that YOLO actually\n",
        "\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
        "\t\t\t# box followed by the boxes' width and height\n",
        "\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
        "\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
        "\t\t\t# and left corner of the bounding box\n",
        "\t\t\tx = int(centerX - (width / 2))\n",
        "\t\t\ty = int(centerY - (height / 2))\n",
        "\t\t\t# update our list of bounding box coordinates, confidences,\n",
        "\t\t\t# and class IDs\n",
        "\t\t\tboxes.append([x, y, int(width), int(height)])\n",
        "\t\t\tconfidences.append(float(confidence))\n",
        "\t\t\tclassIDs.append(classID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHZUDeoHotJ2"
      },
      "source": [
        "**Apply non-maxima suppression** (NMS) to the lists of boxes and confidences. <br>\n",
        "Remove any boxes that do not meet the threshold for overlap and confidence score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MH_DPsKq7B0"
      },
      "source": [
        "# apply non-maxima suppression to suppress weak, overlapping bounding\n",
        "# boxes\n",
        "idxs = cv2.dnn.NMSBoxes(boxes, confidences, .50,0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj7m9pFXpDpM"
      },
      "source": [
        "**Add the boxes, confidence scores, and class IDS to the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzMvrkc-rASz"
      },
      "source": [
        "# ensure at least one detection exists\n",
        "if len(idxs) > 0:\n",
        "\t# loop over the indexes we are keeping\n",
        "\tfor i in idxs.flatten():\n",
        "\t\t# extract the bounding box coordinates\n",
        "\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
        "\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
        "\t\t# draw a bounding box rectangle and label on the image\n",
        "\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
        "\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
        "\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t\t0.5, color, 2)\n",
        "# show the output image\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}