{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV6 ObjectRecognition.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNzSo9wbVPoOPeMP5RJnHa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV6_ObjectRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU3W96NJYgnX"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/computer-vision.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2TVIk1pNOjG"
      },
      "source": [
        "**Import the required libraries and check for GPUs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCgk8hj0rUTc"
      },
      "source": [
        "# For running inference on the TF-Hub module.\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# For downloading the image.\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "# For drawing onto the image.\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# For measuring the inference time.\n",
        "import time\n",
        "\n",
        "# Print Tensorflow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Check available GPU devices.\n",
        "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtHC2LM-OdRQ"
      },
      "source": [
        "# **Faster R-CNN with Inception Resnet v2**\n",
        "With the rise of autonomous vehicles, smart video surveillance, facial detection and various people counting applications, fast and accurate object detection systems are rising in demand. These systems involve not only recognizing and classifying every object in an image, but localizing each one by drawing the appropriate bounding box around it. This makes object detection a significantly harder task than its traditional computer vision predecessor, image classification.<bR>\n",
        "This notebook uses the Faster RCNN with Inception Resnet v2 to perform object detection.<br>\n",
        "\n",
        "*R-CNN == \"Regions with CNN Features\" or \"Region Based CNN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO87X3mwQar-"
      },
      "source": [
        "Faster R-CNN may not be the simplest or fastest method for object detection, but it is still one of the best performing. Case in point, **Tensorflow’s Faster R-CNN with Inception ResNet is their slowest but most accurate model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNsSUWqiO8kq"
      },
      "source": [
        "Object detection API for Tensorflow has pre-built architectures and weights for a few specific models:<br>\n",
        "- Single Shot Multibox Detector (SSD) with MobileNets<br>\n",
        "- SSD with Inception V2<br>\n",
        "- Region-Based Fully Convolutional Networks (R-FCN) with Resnet 101<br>\n",
        "- Faster RCNN with Resnet 101<br>\n",
        "- Faster RCNN with Inception Resnet v2<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofELeue2aXVG"
      },
      "source": [
        "# **R-CNN Modules**<br>\n",
        "R-CNN model is comprised of three modules:<br>\n",
        "\n",
        "- Module 1: Region Proposal. Generate and extract category independent region proposals, e.g. candidate bounding boxes.\n",
        "- Module 2: Feature Extractor. Extract feature from each candidate region, e.g. using a deep convolutional neural network.\n",
        "- Module 3: Classifier. Classify features as one of the known class, e.g. linear SVM classifier model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHzLVmLRbez9"
      },
      "source": [
        "image = cv2.imread(\"images/R-CNN.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnyyBtM-c8kf"
      },
      "source": [
        "# **Fast R-CNN**\n",
        "The architecture of the Fast R-CNN model accepts the photograph as a set of region proposals as input that are passed through a deep convolutional neural network. A pre-trained CNN, such as a VGG-16, is used for feature extraction. The end of the deep CNN is a custom layer called a Region of Interest Pooling Layer, or **RoI Pooling**, that extracts features specific for a given input candidate region.\n",
        "\n",
        "The output of the CNN is then interpreted by a fully connected layer then the model bifurcates into two outputs, one for the class prediction via a softmax layer, and another with a linear output for the bounding box. This process is then repeated multiple times for each region of interest in a given image.\n",
        "\n",
        "The model is significantly faster to train and to make predictions, yet still requires a set of candidate regions to be proposed along with each input image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZt5Gw__Qunq"
      },
      "source": [
        "image = cv2.imread(\"images/FasterRCNN.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9mIZ9q-d6im"
      },
      "source": [
        "image = cv2.imread(\"images/comparisonCNNs.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1USTDY8eQCr"
      },
      "source": [
        "# **Faster CNNs**\n",
        "Faster CNNs have 3 parts:<br>\n",
        "1. Convolution layers - similiar to VGG-16 \n",
        "2. Region Proposal Network (RPN)\n",
        "3. Classes and Bounding Boxes prediction - a fully connected NN to predict the classes and bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD7FWRhFfAZY"
      },
      "source": [
        "image = cv2.imread(\"images/FasterCNNparts.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY9XJ3HegMfm"
      },
      "source": [
        "**RPN**<br>\n",
        "The RPN is a small neural network that uses anchor boxes to predict whether there is an object or not and then predict the bounding box of those objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBObFCZmgA2e"
      },
      "source": [
        "image = cv2.imread(\"images/RegionProposalNetwork.jpeg\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11K39TRDrlen"
      },
      "source": [
        "https://www.tensorflow.org/hub/tutorials/object_detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj4TL58eNarQ"
      },
      "source": [
        "**Helper functions for working with the images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAgkrXbKrsfQ"
      },
      "source": [
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256,\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color,\n",
        "                               font,\n",
        "                               thickness=4,\n",
        "                               display_str_list=()):\n",
        "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(min(boxes.shape[0], max_boxes)):\n",
        "    if scores[i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
        "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
        "                                     int(100 * scores[i]))\n",
        "      color = colors[hash(class_names[i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhxYPBfgNl5v"
      },
      "source": [
        "**Get and resize the image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmH8Kpl1rwMA"
      },
      "source": [
        "# By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg\"  \n",
        "downloaded_image_path = download_and_resize_image(image_url, 1280, 856, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-u0EPkKNtSg"
      },
      "source": [
        "**FasterRCNN+InceptionResNetv2**<br>\n",
        "The model used for this notebook is FasterRCNN+InceptionResNetv2<br>\n",
        "<br>\n",
        "In the image below: <br>\n",
        "- x axis is time\n",
        "- y axis is the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fZZ0sWue3sW"
      },
      "source": [
        "image = cv2.imread(\"images/Accuracy-vs-time-with-marker-shapes-indicating-meta-architecture-and-colors-indicating.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqdaj_dGn5gY"
      },
      "source": [
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\" \n",
        "detector = hub.load(module_handle).signatures['default']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytq1ydwhoisM"
      },
      "source": [
        "The image below is the Inception ResNet v2 architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaPpHoZJr0Jh"
      },
      "source": [
        "image = cv2.imread(\"images/inceptionResNetV2.png\")\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqHv5I5HsJd6"
      },
      "source": [
        "def load_img(path):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8T9EPPtsL_0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def run_detector(detector, path):\n",
        "  img = load_img(path)\n",
        "\n",
        "  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "  start_time = time.time()\n",
        "  result = detector(converted_img)\n",
        "  end_time = time.time()\n",
        "\n",
        "  result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "  print(\"Inference time: \", end_time-start_time)\n",
        "\n",
        "  ds1 = pd.Series(data=result[\"detection_scores\"])\n",
        "  ds2 = pd.Series(result[\"detection_class_labels\"])\n",
        "  ds3 = pd.Series(result[\"detection_class_entities\"])\n",
        "  frame = {'Scores':ds1,'Labels':ds2, \"Class\":ds3}\n",
        "  df = pd.DataFrame(frame)\n",
        "\n",
        "  image_with_boxes = draw_boxes(\n",
        "      img.numpy(), result[\"detection_boxes\"],\n",
        "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
        "\n",
        "  display_image(image_with_boxes)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSQ324t3sOdi"
      },
      "source": [
        "dataframe = run_detector(detector, downloaded_image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfqfGP7bZ091"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataframe.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9DvfWRWsbHx"
      },
      "source": [
        "image_urls = [\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  \"https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg\",\n",
        "  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg\",\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  \"https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg\",\n",
        "  ]\n",
        "\n",
        "def detect_img(image_url):\n",
        "  start_time = time.time()\n",
        "  image_path = download_and_resize_image(image_url, 640, 480)\n",
        "  run_detector(detector, image_path)\n",
        "  end_time = time.time()\n",
        "  print(\"Inference time:\",end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EUs4tqWsdeR"
      },
      "source": [
        "detect_img(image_urls[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sRmRefAsixp"
      },
      "source": [
        "detect_img(image_urls[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axVGFliDsmh9"
      },
      "source": [
        "detect_img(image_urls[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omnuwUycpTwB"
      },
      "source": [
        "# **Assignment**\n",
        "Find a photo on the internet<br>\n",
        "Upload it to the CoLab file system<br>\n",
        "Use the model to do object recognition on the photo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXZHwDKtpkMu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}