{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPGlWTB1vF9MmeR8U5hNXQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/computer-vision/blob/master/CV2_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SJpwj0x7J4K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 # openCV\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=[12,8]\n",
        "\n",
        "# check the opencv version\n",
        "print(cv2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/Masterx-AI/Project_Object_Detection_Yolo_V4/main/people_bicycles.jpg'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('people_bicycles.jpg', 'wb').write(r.content)"
      ],
      "metadata": {
        "id": "Vv3dJ8Zh7QLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the image file\n",
        "test_img = cv2.imread('people_bicycles.jpg')\n",
        "img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Plotting the image\n",
        "def plot_image(img, cmap=None):\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "plot_image(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "veFEPVJc7TnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first create a directory to store the model\n",
        "%mkdir model\n",
        "\n",
        "# enter the directory and download the necessary files\n",
        "%cd model\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/coco.names\n",
        "%cd .."
      ],
      "metadata": {
        "id": "84kc_PCf7XWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to blob object\n",
        "\n",
        "scalefactor = 1.0/255.0\n",
        "new_size = (416, 416)\n",
        "blob = cv2.dnn.blobFromImage(test_img, scalefactor, new_size, swapRB=True, crop=False)"
      ],
      "metadata": {
        "id": "MlHEtOp_7blU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define class labels\n",
        "class_labels_path = \"model/coco.names\"\n",
        "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
        "class_labels\n",
        "\n",
        "# declare repeating bounding box colors for each class\n",
        "# 1st: create a list colors as an RGB string array\n",
        "# Example: Red, Green, Blue, Yellow, Magenda\n",
        "class_colors = [\"255,0,0\",\"0,255,0\",\"0,0,255\",\"255,155,0\",\"255,0, 255\"]\n",
        "\n",
        "#2nd: split the array on comma-separated strings and for change each string type to integer\n",
        "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
        "\n",
        "#3rd: convert the array or arrays to a numpy array\n",
        "class_colors = np.array(class_colors)\n",
        "\n",
        "#4th: tile this to get 80 class colors, i.e. as many as the classes(16 rows of 5cols each).\n",
        "# If you want unique colors for each class you may randomize the color generation or set them manually\n",
        "class_colors = np.tile(class_colors,(16,1))\n",
        "\n",
        "def colored(r, g, b, text):\n",
        "    return \"\\033[38;2;{};{};{}m{} \\033[38;2;255;255;255m\".format(r, g, b, text)\n",
        "\n",
        "for i in range(16):\n",
        "    line = \"\"\n",
        "    for j in range(5):\n",
        "        class_id = i*5 + j\n",
        "        class_id_str = str(class_id)\n",
        "        text = \"class\" + class_id_str\n",
        "        colored_text = colored(class_colors[class_id][0], class_colors[class_id][1], class_colors[class_id][2], text)\n",
        "        line += colored_text\n",
        "    print(line)\n",
        "\n",
        "# or select the colors randomly\n",
        "class_colors = np.random.randint(0, 255, size=(len(class_labels), 3), dtype=\"uint8\")"
      ],
      "metadata": {
        "id": "zzrveYss7go_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "yolo_model = cv2.dnn.readNetFromDarknet('model/yolov4.cfg','model/yolov4.weights')\n",
        "\n",
        "# Read the network layers/components. The YOLO V4 neural network has 379 components. They consist of convolutional layers (conv), rectifier linear units (relu) etc.:\n",
        "model_layers = yolo_model.getLayerNames()\n",
        "print(\"number of network components: \" + str(len(model_layers)))\n",
        "# print(model_layers)\n",
        "\n",
        "# extract the output layers in the code that follows:\n",
        "# - model_layer[0]: returns the index of each output layer in the range of 1 to 379\n",
        "# - model_layer[0] - 1: corrects  this to the range of 0 to 378\n",
        "# - model_layers[model_layer[0] - 1]: returns the indexed layer name\n",
        "output_layers = [model_layers[model_layer - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]\n",
        "\n",
        "# YOLOv4 deploys the same YOLO head as YOLOv3 for detection with the anchor based detection steps, and three levels of detection granularity.\n",
        "print(output_layers)"
      ],
      "metadata": {
        "id": "FCXQCTvV7jxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input pre-processed blob into the model\n",
        "yolo_model.setInput(blob)\n",
        "\n",
        "# compute the forward pass for the input, storing the results per output layer in a list\n",
        "obj_detections_in_layers = yolo_model.forward(output_layers)\n",
        "\n",
        "# verify the number of sets of detections\n",
        "print(\"number of sets of detections: \" + str(len(obj_detections_in_layers)))\n"
      ],
      "metadata": {
        "id": "p_qso8_L7nDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection_analysis(test_image, obj_detections_in_layers, confidence_threshold):\n",
        "\n",
        "  # get the image dimensions\n",
        "  img_height = test_img.shape[0]\n",
        "  img_width = test_img.shape[1]\n",
        "\n",
        "  result = test_image.copy()\n",
        "\n",
        "  # loop over each output layer\n",
        "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
        "    # loop over the detections in each layer\n",
        "      for object_detection in object_detections_in_single_layer:\n",
        "        # obj_detection[1]: bbox center pt_x\n",
        "        # obj_detection[2]: bbox center pt_y\n",
        "        # obj_detection[3]: bbox width\n",
        "        # obj_detection[4]: bbox height\n",
        "        # obj_detection[5]: confidence scores for all detections within the bbox\n",
        "\n",
        "        # get the confidence scores of all objects detected with the bounding box\n",
        "        prediction_scores = object_detection[5:]\n",
        "        # consider the highest score being associated with the winning class\n",
        "        # get the class ID from the index of the highest score\n",
        "        predicted_class_id = np.argmax(prediction_scores)\n",
        "        # get the prediction confidence\n",
        "        prediction_confidence = prediction_scores[predicted_class_id]\n",
        "\n",
        "        # consider object detections with confidence score higher than threshold\n",
        "        if prediction_confidence > confidence_threshold:\n",
        "            # get the predicted label\n",
        "            predicted_class_label = class_labels[predicted_class_id]\n",
        "            # compute the bounding box coordinates scaled for the input image\n",
        "            # scaling is a multiplication of the float coordinate with the appropriate  image dimension\n",
        "            bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
        "            # get the bounding box centroid (x,y), width and height as integers\n",
        "            (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
        "            # to get the start x and y coordinates we to subtract from the centroid half the width and half the height respectively\n",
        "            # for even values of width and height of bboxes adjacent to the  image border\n",
        "            #  this may generate a -1 which is prevented by the max() operator below\n",
        "            start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
        "            start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
        "            end_x_pt = start_x_pt + box_width\n",
        "            end_y_pt = start_y_pt + box_height\n",
        "\n",
        "            # get a random mask color from the numpy array of colors\n",
        "            box_color = class_colors[predicted_class_id]\n",
        "\n",
        "            # convert the color numpy array as a list and apply to text and box\n",
        "            box_color = [int(c) for c in box_color]\n",
        "\n",
        "            # print the prediction in console\n",
        "            predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
        "            print(\"predicted object {}\".format(predicted_class_label))\n",
        "\n",
        "            # draw the rectangle and text in the image\n",
        "            cv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
        "            cv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
        "  return result\n",
        "\n",
        "confidence_threshold = 0.2\n",
        "result_raw = object_detection_analysis(test_img, obj_detections_in_layers, confidence_threshold)\n",
        "\n",
        "result_img = cv2.cvtColor(result_raw, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(result_img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dv9jGxhB7uGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids_list = []\n",
        "boxes_list = []\n",
        "confidences_list = []"
      ],
      "metadata": {
        "id": "5-AfSfBU7y1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection_attributes(test_image, obj_detections_in_layers, confidence_threshold):\n",
        "  # get the image dimensions\n",
        "  img_height = test_img.shape[0]\n",
        "  img_width = test_img.shape[1]\n",
        "\n",
        "  # loop over each output layer\n",
        "  for object_detections_in_single_layer in obj_detections_in_layers:\n",
        "    # loop over the detections in each layer\n",
        "    for object_detection in object_detections_in_single_layer:\n",
        "      # get the confidence scores of all objects detected with the bounding box\n",
        "      prediction_scores = object_detection[5:]\n",
        "      # consider the highest score being associated with the winning class\n",
        "      # get the class ID from the index of the highest score\n",
        "      predicted_class_id = np.argmax(prediction_scores)\n",
        "      # get the prediction confidence\n",
        "      prediction_confidence = prediction_scores[predicted_class_id]\n",
        "\n",
        "      # consider object detections with confidence score higher than threshold\n",
        "      if prediction_confidence > confidence_threshold:\n",
        "        # get the predicted label\n",
        "        predicted_class_label = class_labels[predicted_class_id]\n",
        "        # compute the bounding box coordinates scaled for the input image\n",
        "        bounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
        "        (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
        "        start_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
        "        start_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
        "\n",
        "        # update the 3 lists for nms processing\n",
        "        # - confidence is needed as a float\n",
        "        # - the bbox info has the openCV Rect format\n",
        "        class_ids_list.append(predicted_class_id)\n",
        "        confidences_list.append(float(prediction_confidence))\n",
        "        boxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])\n"
      ],
      "metadata": {
        "id": "pqcvMNg672qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_threshold = 0.5\n",
        "object_detection_attributes(test_img, obj_detections_in_layers, score_threshold)"
      ],
      "metadata": {
        "id": "91ogr-GR776D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NMS for a set of overlapping bboxes returns the ID of the one with highest\n",
        "# confidence score while suppressing all others (non maxima)\n",
        "# - score_threshold: a threshold used to filter boxes by score\n",
        "# - nms_threshold: a threshold used in non maximum suppression.\n",
        "\n",
        "score_threshold = 0.5\n",
        "nms_threshold = 0.4\n",
        "winner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)\n",
        "\n",
        "# loop through the final set of detections remaining after NMS and draw bounding box and write text\n",
        "for winner_id in winner_ids:\n",
        "    max_class_id = winner_id\n",
        "    box = boxes_list[max_class_id]\n",
        "    start_x_pt = box[0]\n",
        "    start_y_pt = box[1]\n",
        "    box_width = box[2]\n",
        "    box_height = box[3]\n",
        "\n",
        "    #get the predicted class id and label\n",
        "    predicted_class_id = class_ids_list[max_class_id]\n",
        "    predicted_class_label = class_labels[predicted_class_id]\n",
        "    prediction_confidence = confidences_list[max_class_id]\n",
        "    #obtain the bounding box end coordinates\n",
        "    end_x_pt = start_x_pt + box_width\n",
        "    end_y_pt = start_y_pt + box_height\n",
        "\n",
        "    #get a random mask color from the numpy array of colors\n",
        "    box_color = class_colors[predicted_class_id]\n",
        "\n",
        "    #convert the color numpy array as a list and apply to text and box\n",
        "    box_color = [int(c) for c in box_color]\n",
        "\n",
        "    # print the prediction in console\n",
        "    predicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
        "    print(\"predicted object {}\".format(predicted_class_label))\n",
        "\n",
        "    # draw rectangle and text in the image\n",
        "    cv2.rectangle(test_img, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 2)\n",
        "    cv2.putText(test_img, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)\n",
        "test_imgz = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(test_imgz)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SO3TcCuZ79LA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}